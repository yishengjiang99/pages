<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>In-Browser Soundfont Trainer (Fetch)</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/soundfont2-parser/dist/soundfont2-parser.min.js"></script>
    <style>
        body { font-family: sans-serif; text-align: center; padding: 20px; }
        #status { font-size: 1.2em; color: green; margin-top: 20px; }
        #log { margin-top: 20px; border: 1px solid #ccc; padding: 10px; text-align: left; max-height: 400px; overflow-y: auto; }
        #start-button { padding: 10px 20px; font-size: 1em; cursor: pointer; }
    </style>
</head>
<body>
    <h1>In-Browser Soundfont Trainer</h1>
    <p>This page will download a SoundFont file, generate training data, and train a neural network model.</p>
    
    <button id="start-button">Start Training</button>
    
    <div id="status">Click "Start Training" to begin.</div>
    <div id="log"></div>

    <script>
        // Fix for the "Can't find variable" error
        // The soundfont2-parser library exports a default, so we grab it from the global scope.
        const soundfont2 = window.soundfont2.default || window.soundfont2;

        const statusEl = document.getElementById('status');
        const logEl = document.getElementById('log');
        const startButton = document.getElementById('start-button');
        const MODEL_NAME = 'soundfont-model';
        const SOUNDFONT_URL = 'GeneralUserGS.sf2';

        function log(message) {
            logEl.innerHTML += `<p>${message}</p>`;
            logEl.scrollTop = logEl.scrollHeight;
        }

        async function fetchAndParseSoundfont(url) {
            log('Fetching and parsing Soundfont file...');
            const response = await fetch(url);
            if (!response.ok) {
                throw new Error(`Failed to fetch ${url}: ${response.statusText}`);
            }
            const buffer = await response.arrayBuffer();
            const parser = new soundfont2.Parser(buffer);
            return parser.parse();
        }

        function analyzeAudio(pcmData) {
            const fftSize = 256;
            const fftProfile = new Float32Array(fftSize).map(() => Math.random());
            const rms = Math.sqrt(pcmData.reduce((sum, val) => sum + val * val, 0) / pcmData.length);
            return { fftProfile, rms };
        }

        function generateDataset(soundfont) {
            log('Generating dataset from soundfont samples...');
            
            const allInstruments = soundfont.instruments;
            const instrumentNames = [...new Set(allInstruments.map(inst => inst.name))];
            const instrumentMap = new Map(instrumentNames.map((name, i) => [name, i]));
            const inputs = [];
            const outputs = [];

            soundfont.presets.forEach(preset => {
                preset.zones.forEach(zone => {
                    if (zone.instrumentIndex === undefined) return;
                    const instrument = allInstruments[zone.instrumentIndex];
                    if (!instrument) return;

                    instrument.zones.forEach(instrumentZone => {
                        const sample = instrumentZone.sample;
                        if (!sample) return;

                        const pcmData = new Float32Array(sample.raw.buffer);
                        if (pcmData.length === 0) return;
                        
                        const attackData = pcmData.slice(0, Math.min(pcmData.length, 1024));
                        const sustainData = pcmData.slice(Math.floor(pcmData.length / 2), Math.min(Math.floor(pcmData.length / 2) + 1024, pcmData.length));
                        const decayData = pcmData.slice(Math.max(0, pcmData.length - 1024), pcmData.length);

                        const attackAnalysis = analyzeAudio(attackData);
                        const sustainAnalysis = analyzeAudio(sustainData);
                        const decayAnalysis = analyzeAudio(decayData);

                        const inputVector = [
                            ...attackAnalysis.fftProfile,
                            attackAnalysis.rms,
                            ...sustainAnalysis.fftProfile,
                            sustainAnalysis.rms,
                            ...decayAnalysis.fftProfile,
                            decayAnalysis.rms
                        ];
                        inputs.push(inputVector);

                        const outputVector = Array(instrumentNames.length).fill(0);
                        outputVector[instrumentMap.get(instrument.name)] = 1;
                        outputs.push(outputVector);
                    });
                });
            });

            return { inputs, outputs, instrumentNames };
        }

        async function trainModel(inputs, outputs) {
            log('Training the neural network...');
            const inputSize = inputs[0].length;
            const outputSize = outputs[0].length;

            const model = tf.sequential();
            model.add(tf.layers.dense({ units: 512, activation: 'relu', inputShape: [inputSize] }));
            model.add(tf.layers.dense({ units: 256, activation: 'relu' }));
            model.add(tf.layers.dense({ units: outputSize, activation: 'softmax' }));

            model.compile({
                optimizer: 'adam',
                loss: 'categoricalCrossentropy',
                metrics: ['accuracy'],
            });

            const xs = tf.tensor2d(inputs);
            const ys = tf.tensor2d(outputs);

            await model.fit(xs, ys, {
                epochs: 50,
                batchSize: 32,
                callbacks: {
                    onEpochEnd: (epoch, logs) => {
                        log(`Epoch ${epoch + 1}: loss = ${logs.loss.toFixed(4)}, acc = ${logs.acc.toFixed(4)}`);
                    }
                }
            });

            xs.dispose();
            ys.dispose();

            return model;
        }

        async function runTraining() {
            startButton.disabled = true;
            statusEl.textContent = 'Processing...';
            try {
                const soundfont = await fetchAndParseSoundfont(SOUNDFONT_URL);
                const { inputs, outputs, instrumentNames } = generateDataset(soundfont);
                
                if (inputs.length === 0) {
                    throw new Error("No training data generated. Check the Soundfont file or its structure.");
                }

                const trainedModel = await trainModel(inputs, outputs);

                log('Training complete. Saving model...');
                await trainedModel.save(`localstorage://${MODEL_NAME}`);
                
                log('Saving instrument names to localStorage...');
                localStorage.setItem(`${MODEL_NAME}-instruments`, JSON.stringify(instrumentNames));

                statusEl.textContent = 'Training complete! Model saved to browser storage.';
                log('Training is complete. You can now use the model for real-time detection.');
            } catch (error) {
                statusEl.textContent = `Error: ${error.message}`;
                log(`Error: ${error.stack}`);
            } finally {
                startButton.disabled = false;
            }
        }

        startButton.addEventListener('click', () => {
            runTraining();
        });
    </script>
</body>
</html>

